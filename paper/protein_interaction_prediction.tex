\documentclass[12pt,a4paper]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[table]{xcolor}
\usepackage{subcaption}
\usepackage{float}

\geometry{margin=1in}
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}
\graphicspath{{figures/}}

\title{Predicting Protein-Protein Interactions using Graph Neural Networks:\\A Graph-Based Deep Learning Approach}
\author{Your Name \\
  Department of Computer Science \\
  University Name \\
  Email: your.email@university.edu}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Protein-protein interactions (PPIs) form the foundation of cellular processes and understanding these interactions is crucial for therapeutic development and systems biology. Despite their importance, traditional experimental methods for detecting PPIs remain time-consuming and resource-intensive, creating a pressing need for computational approaches. This research presents a novel graph neural network-based approach that leverages both protein sequence and structural information to predict PPIs. Our method represents proteins as graphs where amino acid residues are nodes and their relationships form edges. We employ both Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs) to learn meaningful representations for interaction prediction. By utilising protein sequence embeddings from ProtBERT as initial node features, we demonstrate that our approach achieves competitive performance on multiple datasets, including human PPIs, yeast PPIs, and SARS-CoV-2-human protein interactions. Furthermore, we analyse the attention mechanisms to identify key residues involved in the interactions, providing valuable biological insights beyond binary classification. Our findings suggest that graph-based deep learning offers a promising avenue for understanding protein interactions at both molecular and systems levels.
\end{abstract}

\section{Introduction}

Protein-protein interactions (PPIs) constitute the fundamental machinery underlying cellular processes, including signal transduction, metabolic pathways, gene regulation, and structural organisation. Understanding these interactions is essential for elucidating disease mechanisms, identifying drug targets, and engineering proteins with desired functions \cite{gonzalez2012protein}. Traditional experimental methods for PPI detection, such as yeast two-hybrid assays and co-immunoprecipitation, whilst accurate, are notably laborious, expensive, and difficult to scale to proteome-wide analyses \cite{berggard2007methods}.

Computational methods offer a complementary approach for predicting PPIs, enabling high-throughput screening and prioritisation of candidate interactions for experimental validation \cite{skrabanek2008computational}. Early computational approaches relied heavily on sequence homology, domain knowledge, and co-evolution signals \cite{valencia2002computational}. More recently, machine learning methods, particularly deep learning, have demonstrated promising results in PPI prediction by automatically learning relevant features from sequence and structural data \cite{chen2019review}.

In this research, we propose a graph-based representation of proteins, where amino acid residues are nodes and their spatial or sequential relationships form the edges. This representation elegantly preserves the complex three-dimensional structure of proteins whilst enabling efficient computation. We employ two types of graph neural networks: Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs) to learn meaningful representations for PPI prediction.

Our contributions are threefold:
\begin{enumerate}
    \item We propose a comprehensive framework for representing proteins as graphs using sequence and structural information, capturing both local and global contextual features.
    \item We develop and evaluate GCN and GAT models for predicting protein-protein interactions and demonstrate their effectiveness on multiple biologically relevant datasets.
    \item We analyse the attention mechanisms in the GAT model to provide interpretable insights into the key residues involved in protein interactions, bridging the gap between prediction and biological understanding.
\end{enumerate}

The remainder of this thesis is organised as follows: Section 2 reviews related work in PPI prediction. Section 3 describes the datasets used in our experiments. Section 4 details our methodology, including protein graph construction and neural network architectures. Section 5 presents the experimental results and evaluation metrics. Section 6 analyses the model interpretability and discusses biological insights. Finally, Section 7 concludes the thesis and suggests directions for future work.

\section{Related Work}

\subsection{Traditional Approaches to PPI Prediction}

Traditional computational methods for PPI prediction rely on various biological features, including sequence homology, gene co-expression, phylogenetic profiles, and structural information \cite{valencia2002computational}. These methods often require extensive domain knowledge and feature engineering. For instance, sequence-based methods analyse amino acid compositions, evolutionary conservation, and domain information \cite{shen2007predicting}. Structure-based methods focus on surface complementarity and binding site characteristics \cite{huang2015scoring}. Whilst these approaches have contributed significantly to our understanding of protein interactions, they often struggle with the inherent complexity and context-dependent nature of PPIs.

\subsection{Machine Learning for PPI Prediction}

Machine learning has gained popularity for PPI prediction due to its ability to automatically learn relevant features from data. Support Vector Machines (SVMs), Random Forests, and Neural Networks have been applied using various protein representations \cite{you2013prediction}. These approaches typically convert protein sequences into fixed-length feature vectors through techniques like conjoint triads \cite{shen2007predicting} or auto-covariance \cite{guo2008using}. Whilst these methods have demonstrated good performance on benchmark datasets, they often fail to capture the structural nuances and long-range dependencies in protein sequences that are crucial for interaction prediction.

\subsection{Deep Learning Approaches}

Recent advances in deep learning have ushered in a new era of PPI prediction methods. Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have been employed to capture local and sequential patterns in protein sequences \cite{sun2017sequence}. Models like PIPR \cite{chen2019multifaceted} and DeepPPI \cite{du2017deepppi} use multi-channel CNNs to extract features from different aspects of protein pairs. These deep learning approaches have significantly improved prediction accuracy but still face challenges in representing the complex, hierarchical nature of protein structures and interactions.

\subsection{Graph Neural Networks in Computational Biology}

Graph Neural Networks (GNNs) have emerged as a particularly powerful paradigm for modelling relational data in computational biology. In protein science, GNNs have been used for protein structure prediction \cite{ingraham2019generative}, function annotation \cite{gligorijevic2021structure}, and more recently, PPI prediction \cite{fout2017protein}. GNNs offer a natural way to represent proteins as graphs, preserving their structural characteristics whilst enabling efficient computation. Our work builds upon these advances by developing specialised GNN architectures for PPI prediction and providing interpretable insights into the molecular basis of protein interactions.

\section{Datasets}
\label{sec:datasets}

In this study, we utilise three primary datasets to train and evaluate our models, each offering unique perspectives on protein-protein interactions:

\subsection{Human PPI Dataset}

The Human PPI dataset represents a comprehensive collection of experimentally validated protein-protein interactions in humans. This dataset is crucial for understanding the complex interactome of human cells and identifying potential drug targets for various diseases.

\textbf{Source:} The dataset is compiled from multiple authoritative repositories, including the Human Protein Reference Database (HPRD) \cite{keshava2009human}, the Biological General Repository for Interaction Datasets (BioGRID) \cite{stark2006biogrid}, and the International Molecular Exchange Consortium (IntAct) \cite{orchard2014mintact}.

\textbf{Composition:} The dataset includes positive samples consisting of experimentally confirmed interacting protein pairs, as well as negative samples comprising protein pairs unlikely to interact, based on subcellular localisation evidence. For each protein, we retained the amino acid sequence and computed embeddings to facilitate model training.

\textbf{Size:} The processed dataset contains thousands of protein pairs with a balanced distribution of positive and negative examples, ensuring robust model training and evaluation.

\subsection{S. cerevisiae (Yeast) Dataset}

Saccharomyces cerevisiae, commonly known as baker's yeast, is a model organism with a well-studied proteome, making it an ideal benchmark for PPI prediction methods. Its relative simplicity compared to human cells, combined with extensive experimental data, provides a valuable resource for method development and validation.

\textbf{Source:} The dataset is derived from the Database of Interacting Proteins (DIP) \cite{salwinski2004database} and supplemented with data from the Saccharomyces Genome Database (SGD) \cite{cherry2012saccharomyces}.

\textbf{Composition:} Similar to the human dataset, this collection includes experimentally verified interacting yeast protein pairs as positive samples. The negative samples comprise randomly selected protein pairs with different subcellular localisations, enhancing the biological relevance of our evaluation.

\textbf{Size:} The processed dataset maintains a balanced set of positive and negative interaction pairs, ensuring fair evaluation of our prediction methods.

\subsection{COVID-19 BioGRID Dataset}

The COVID-19 BioGRID dataset focuses on interactions between SARS-CoV-2 proteins and human host proteins, which is crucial for understanding viral pathogenesis and developing therapeutic interventions. This dataset is particularly relevant given the recent global pandemic and the urgent need for computational methods to accelerate drug discovery efforts.

\textbf{Source:} The dataset is obtained from the COVID-19 Coronavirus Project within BioGRID \cite{oughtred2021biogrid}, which meticulously curates experimentally identified physical interactions between viral and host proteins.

\textbf{Composition:} The dataset includes viral proteins from SARS-CoV-2 (spike, nucleocapsid, membrane, etc.), human host proteins interacting with these viral proteins, and detailed information about the types of interactions identified through various experimental techniques.

\textbf{Processing:} We processed this dataset through several carefully designed steps: extraction of relevant protein pairs from BioGRID, retrieval of protein sequences from UniProt using bespoke sequence fetching utilities, generation of protein embeddings using advanced language models, and conversion of proteins to graph representations using our custom-developed algorithms.

\textbf{Size:} The processed dataset contains hundreds of virus-host protein interactions, complete with protein sequences and their corresponding embeddings, providing a rich resource for studying host-pathogen interaction mechanisms.

\section{Methodology}
\label{sec:methodology}

Our methodology for predicting protein-protein interactions comprises four main phases: (1) data acquisition, (2) feature engineering, (3) model architecture development, and (4) training and evaluation. Figure \ref{fig:pipeline} presents a schematic overview of our experimental pipeline.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/ppi_prediction_pipeline.pdf}
    \caption{Schematic overview of the protein-protein interaction prediction pipeline. The process begins with data acquisition from multiple sources, followed by feature engineering to convert protein sequences into graph representations. Graph neural network models (GCN and GAT) are then trained and evaluated on their ability to predict protein-protein interactions.}
    \label{fig:pipeline}
\end{figure}

\subsection{Protein Sequence Embedding}
\label{sec:protein_embedding}

The foundation of our approach lies in the effective representation of protein sequences. Proteins, as complex biomolecules, possess hierarchical structural organisation that significantly influences their functions and interactions. To capture this rich information, we leverage advances in protein language modelling by employing ProtBERT \cite{elnaggar2021prottrans}, a transformer-based model pre-trained on over 250 million protein sequences. This approach provides contextualised embeddings that capture both evolutionary and functional information encoded in protein sequences.

\subsubsection{ProtBERT Architecture}

ProtBERT represents a significant advancement in protein representation learning, built upon the BERT (Bidirectional Encoder Representations from Transformers) architecture. The model comprises 30 transformer encoder layers with 16 attention heads, generating 1024-dimensional hidden representations for each amino acid. The pre-training process utilises masked language modelling on extensive protein databases including UniRef and BFD, enabling the model to learn the complex grammar of protein sequences.

Unlike traditional sequence embedding methods that treat each amino acid independently, ProtBERT leverages sophisticated self-attention mechanisms to generate contextualised representations that consider the entire protein sequence. This approach allows the model to capture long-range dependencies and implicit structural information from sequence data alone, a capability that is particularly valuable when explicit structural data is unavailable for many proteins of interest.

\subsubsection{Embedding Generation Process}

Our embedding process follows a thoughtfully designed workflow that maximises the information captured from each protein sequence. Initially, each protein sequence undergoes tokenisation, where spaces are inserted between amino acids and special tokens ([CLS] and [SEP]) are added at the beginning and end, respectively, to facilitate processing by the transformer architecture.

The tokenised sequence is then processed through ProtBERT to generate contextual embeddings for each amino acid. Mathematically, given a protein sequence $S = [a_1, a_2, ..., a_L]$ of length $L$, ProtBERT produces an embedding matrix $E \in \mathbb{R}^{L \times d}$ where $d = 1024$, representing each amino acid in its sequential context. This representation preserves the positional information and contextual relationships between amino acids that are critical for protein function.

In the post-processing stage, special token embeddings are removed, and the resulting embeddings are stored along with the original sequence information. This comprehensive approach results in a rich representation that preserves both sequential and implicit structural information, providing a solid foundation for subsequent graph construction.

\subsubsection{Comparative Analysis of Embedding Methods}

Our selection of ProtBERT over alternative protein representation methods was informed by a careful analysis of their respective advantages and limitations. Traditional methods like one-hot encoding, whilst simple to implement, ignore the biochemical similarities between amino acids and result in high-dimensional, sparse representations. Substitution matrix-based approaches (BLOSUM/PAM) capture evolutionary relationships but lack contextual information crucial for understanding protein function.

Word embedding techniques like Word2Vec and GloVe provide fixed-size embeddings and capture local patterns but are limited by their restricted context window. More recent approaches like ESM/ESM-2 offer state-of-the-art performance with various model sizes but can be computationally intensive for larger variants.

ProtBERT strikes an optimal balance, offering contextualised representations that capture long-range dependencies whilst being pre-trained on diverse protein data. Its primary limitations—high memory requirements and computational intensity—were addressed in our implementation through efficient use of hardware acceleration and batch processing strategies.

\subsection{Graph Construction}
\label{sec:graph_construction}

The representation of proteins as graphs constitutes a central innovation in our methodology, enabling us to leverage the inherent structural properties of proteins whilst applying powerful graph-based learning techniques. This section details our graph construction process, which transforms protein sequence embeddings into structured graph representations.

\subsubsection{Node Definition and Features}

In our graph representation, each amino acid residue in a protein is defined as a node, preserving the fundamental building blocks of protein structure. Node features are derived directly from the ProtBERT embeddings described in Section \ref{sec:protein_embedding}, providing a rich characterisation of each residue in its sequential context.

Formally, for a protein graph $G = (V, E, X)$, the set of nodes $V = \{v_1, v_2, \ldots, v_L\}$ corresponds to the amino acid residues, where $|V| = L$ represents the sequence length. The node feature matrix $X \in \mathbb{R}^{L \times d}$ contains the 1024-dimensional embeddings from ProtBERT, capturing both the biochemical properties and contextual information of each residue.

\subsubsection{Edge Construction Strategies}

The definition of edges in our protein graphs represents a critical design choice that significantly influences the model's ability to capture protein structure and function. We explored multiple strategies for edge construction, each offering different perspectives on residue relationships:

Sequential Proximity serves as our primary approach, establishing edges between residues within a defined window size $w$ in the sequence. For residue positions $i$ and $j$, an edge exists if $|i - j| \leq w$. This approach effectively captures the local sequential context of each residue, reflecting the fact that nearby residues in the sequence often form important structural motifs.

The $k$-Nearest Neighbours approach creates edges based on similarity in the embedding space, using measures such as cosine similarity or Euclidean distance. This strategy captures functional similarity beyond sequential proximity, potentially identifying residues that interact in the folded protein despite being distant in the primary sequence.

For proteins with known structures, Contact Maps offer a third approach, creating edges based on physical proximity in three-dimensional space. Residues are connected if their C$\alpha$ or C$\beta$ atoms lie within a specified distance threshold (typically 8-10Å), directly reflecting the actual spatial relationships in the folded protein.

After careful evaluation, we selected sequential proximity with $w = 3$ as our primary method, creating a window of 7 residues centred at each position. This choice balances the capture of local structural motifs whilst maintaining computational efficiency, a critical consideration for processing large protein datasets.

\subsubsection{Graph Construction Implementation}

Our graph construction process transforms the abstract representation described above into concrete graph structures suitable for processing by graph neural networks. The process begins with protein embeddings generated by our embedding module, which are then normalised to ensure consistent scale and improve training stability.

Edge indices are created based on the chosen edge construction strategy, defining the connectivity pattern of the graph. The resulting graph structures are then formatted using the PyTorch Geometric library, which provides efficient data structures and operations for graph-based deep learning.

This careful construction of protein graphs preserves the essential structural and functional features of proteins whilst enabling efficient processing through our graph neural network architectures. The resulting graphs serve as the foundation for our PPI prediction framework, providing a rich representation that captures both local and global protein characteristics.

\subsection{Graph Neural Network Models}
\label{sec:gnn_models}

At the core of our protein-protein interaction prediction framework lie two sophisticated graph neural network architectures: Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs). Both models learn to extract meaningful protein representations from the graph structure and node features but employ fundamentally different mechanisms for message passing and information aggregation.

\subsubsection{Graph Convolutional Network (GCN)}

\paragraph{Theoretical Foundation}

Graph Convolutional Networks represent a principled approach to generalising convolutional operations from regular grid-like structures (such as images) to irregular graph-structured data. In our GCN implementation, the core operation follows the formulation proposed by Kipf and Welling, defined as:

\begin{equation}
    H^{(l+1)} = \sigma\left(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}\right)
\end{equation}

In this formulation, $H^{(l)} \in \mathbb{R}^{n \times d_l}$ represents the matrix of node features at layer $l$, whilst $\tilde{A} = A + I_n$ denotes the adjacency matrix with added self-loops, ensuring that each node's own features are considered in the update. The diagonal degree matrix $\tilde{D}$ normalises the aggregation based on node connectivity, preventing numerical instabilities and ensuring that nodes with many connections do not disproportionately influence the model. The learnable weight matrix $W^{(l)}$ enables the model to transform features appropriately at each layer, whilst the non-linear activation function $\sigma$ (ReLU in our implementation) introduces the necessary complexity to model non-linear relationships.

This sophisticated formulation allows the model to aggregate information from neighbouring nodes whilst accounting for varying node degrees through normalisation, a crucial capability for processing proteins with varying sequence lengths and connectivity patterns.

\paragraph{Model Architecture}

Our GCN model incorporates several architectural choices designed to enhance its representational capacity and training stability. The model begins with node features derived from ProtBERT embeddings, providing a rich initial representation with 1024 dimensions per residue. These features are processed through multiple GCN layers, each followed by batch normalisation to stabilise training and residual connections to facilitate gradient flow in deeper networks.

Global mean pooling across all nodes generates a graph-level representation that captures the overall characteristics of the protein. This pooled representation is then processed through a multi-layer perceptron that predicts the probability of interaction between protein pairs.

The incorporation of residual connections proves particularly valuable in our architecture, allowing the model to preserve information from earlier layers whilst learning increasingly abstract representations. Batch normalisation further enhances training stability by normalising the distribution of layer inputs, addressing the internal covariate shift problem that can impede learning in deep neural networks.

\subsubsection{Graph Attention Network (GAT)}

\paragraph{Theoretical Foundation}

Graph Attention Networks extend GCNs by incorporating attention mechanisms that assign different weights to different neighbouring nodes, allowing the model to focus on the most relevant connections for the task at hand. The key innovation lies in the introduction of self-attention for message passing, which computes attention coefficients as:

\begin{equation}
    \alpha_{ij} = \frac{\exp\left(LeakyReLU\left(\mathbf{a}^T[W\mathbf{h}_i \| W\mathbf{h}_j]\right)\right)}{\sum_{k \in \mathcal{N}(i)} \exp\left(LeakyReLU\left(\mathbf{a}^T[W\mathbf{h}_i \| W\mathbf{h}_k]\right)\right)}
\end{equation}

Here, $\mathbf{h}_i$ represents the feature vector of node $i$, $W$ denotes a learnable linear transformation, and $\mathbf{a}$ is a learnable attention vector. The attention coefficient $\alpha_{ij}$ quantifies the importance of node $j$'s features to node $i$, whilst the neighbourhood $\mathcal{N}(i)$ defines the set of nodes considered in the aggregation. The concatenation operation, represented by $\|$, combines the features of the target and source nodes to compute their relationship strength.

These attention coefficients are then used to compute weighted aggregations of neighbouring node features:

\begin{equation}
    \mathbf{h}_i^{\prime} = \sigma\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij} W \mathbf{h}_j\right)
\end{equation}

To further enhance the model's representational capacity, we employ multi-head attention, where $K$ independent attention mechanisms compute different representations that are then combined:

\begin{equation}
    \mathbf{h}_i^{\prime} = \|_{k=1}^K \sigma\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij}^k W^k \mathbf{h}_j\right)
\end{equation}

This multi-head approach allows the model to jointly attend to information from different representation subspaces, capturing diverse aspects of the node relationships.

\paragraph{Model Architecture}

Our GAT model builds upon these theoretical foundations with a carefully designed architecture optimised for protein representation learning. The model begins with the same ProtBERT embeddings as the GCN but processes them through GAT layers with multi-head attention and residual connections. Specifically, we employ 8 attention heads in each GAT layer, allowing the model to learn diverse relationship patterns between amino acid residues.

A critical innovation in our architecture is the attention-weighted pooling mechanism used to obtain graph-level representations. Unlike the uniform mean pooling used in the GCN, this approach assigns learnable weights to different nodes when computing the graph representation, allowing the model to focus on the most interaction-relevant residues.

The final GAT representations are processed through a multi-layer perceptron similar to the GCN, predicting interaction probabilities for protein pairs. The attention mechanisms throughout the model not only enhance predictive performance but also provide valuable interpretability, highlighting the residues most relevant to the predicted interactions.

\subsubsection{Key Differences Between GCN and GAT}

The choice between GCN and GAT architectures presents a fundamental trade-off in model design. GCNs offer uniform aggregation weighted by node degree, providing a balanced consideration of all neighbouring nodes. This approach is computationally efficient with fewer parameters but may limit the model's expressiveness in capturing complex relationships.

In contrast, GATs employ attention-weighted aggregation, allowing the model to focus on the most relevant neighbours for each node. This increased expressiveness comes at the cost of higher parameter count and computational complexity. Particularly noteworthy is the enhanced interpretability offered by GATs, where attention weights provide insights into important connections that may have biological significance.

Our experimental design incorporates both architectures, allowing us to evaluate these trade-offs empirically and gain insights into the relationship structures most relevant for protein interaction prediction.

\subsection{Protein-Protein Interaction Prediction}
\label{sec:ppi_prediction}

Building upon the protein representations learned by our graph neural networks, we developed a unified framework for predicting interactions between protein pairs. This framework employs a siamese network architecture that processes each protein independently before combining their representations to predict interaction probability.

\subsubsection{Siamese Network Architecture}

Our PPI prediction framework follows a systematic approach designed to capture both individual protein characteristics and their potential for interaction. Initially, each protein in a pair $(P_A, P_B)$ is processed independently through the same graph neural network (either GCN or GAT) to obtain graph-level embeddings $h_A$ and $h_B$. This independent processing ensures that each protein's representation captures its inherent structural and functional properties before considering its interaction potential.

The combination of protein embeddings represents a critical design choice in our framework. We explored multiple strategies for this combination: simple concatenation ($h = [h_A \| h_B]$), element-wise product ($h = h_A \odot h_B$), absolute difference ($h = |h_A - h_B|$), and hybrid combinations that incorporate multiple perspectives ($h = [h_A \| h_B \| h_A \odot h_B \| |h_A - h_B|]$).

After extensive experimentation, we selected concatenation followed by a two-layer MLP with dropout regularisation as our primary implementation. This approach preserves the complete information from both proteins whilst allowing the model to learn complex interaction patterns. The interaction probability is computed as:

\begin{equation}
    p(P_A, P_B) = \sigma(MLP(h))
\end{equation}

where $\sigma$ represents the sigmoid activation function that transforms the MLP output into a probability between 0 and 1.

\subsubsection{Training Objective}

Our models are trained to minimise the binary cross-entropy loss between predicted and actual interactions:

\begin{equation}
    \mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} [y_i \log(p_i) + (1-y_i) \log(1-p_i)]
\end{equation}

where $y_i \in \{0, 1\}$ denotes the ground truth label (1 for interacting pairs, 0 for non-interacting pairs), $p_i$ represents the predicted probability of interaction, and $N$ is the number of protein pairs in the batch.

This loss function provides a principled approach to binary classification, encouraging the model to assign high probabilities to actual interactions and low probabilities to non-interactions. The logarithmic form of the loss function particularly penalises confident but incorrect predictions, guiding the model towards calibrated probability estimates.

\subsection{Data Preparation and Training Pipeline}
\label{sec:pipeline}

The development of a robust training and evaluation pipeline is essential for ensuring reliable model performance and fair comparison between different approaches. Our pipeline encompasses data preparation, model training, and rigorous evaluation procedures.

\subsubsection{Data Preparation}

Our data preparation process begins with the collection of experimentally validated protein interactions from authoritative databases. These positive samples are complemented by carefully constructed negative samples, created by random pairing of proteins that are not known to interact. To enhance the biological relevance of these negative examples, we consider subcellular localisation, ensuring that proteins from entirely different cellular compartments (which would rarely have opportunity to interact) are well-represented in the negative set.

Balance between positive and negative samples is maintained throughout our datasets, preventing biased learning that could favour one class over the other. This balanced approach is particularly important in PPI prediction, where the natural distribution of interactions is highly skewed towards non-interactions.

The prepared data is split into training (70\%), validation (15\%), and test (15\%) sets using stratification to maintain class balance across splits. This partitioning enables rigorous evaluation of model performance on unseen data whilst providing a validation set for hyperparameter tuning and early stopping.

\subsubsection{Training Pipeline}

Our training pipeline is designed to maximise model performance whilst ensuring fair evaluation and reproducibility. The process begins with model initialisation, creating an instance of either the GCN or GAT model with appropriate hyperparameters determined through preliminary experiments.

We implement several optimisation strategies to enhance training effectiveness. Multiple optimiser options (Adam, Stochastic Gradient Descent, Adagrad, and RAdam) are supported, with RAdam emerging as our preferred choice due to its adaptive learning rate properties and improved convergence behaviour. Learning rate schedulers (multi-step, plateau-based, and step decay) provide automatic adjustment of learning rates during training, helping the model navigate the complex loss landscape effectively.

The training loop processes batches of protein pairs through the model, calculates loss, performs backpropagation, and updates model parameters. Regular evaluation on the validation set monitors performance and guides early stopping decisions, preventing overfitting by terminating training when validation performance fails to improve for a specified number of epochs.

Model selection is based on validation performance, with the best-performing model saved for final evaluation on the test set. This comprehensive approach ensures that our reported results reflect true generalisation performance rather than overfitting to training data.

\subsubsection{Hyperparameter Configuration}

The performance of deep learning models depends critically on their hyperparameter configuration. Through extensive experimentation, we identified optimal hyperparameter settings for both our GCN and GAT models.

For the GCN model, we employ an input dimension of 1024 (matching the ProtBERT embeddings), hidden dimension of 256, and output dimension of 128. The model comprises 3 GCN layers with dropout rate of 0.5 for regularisation. Training uses a batch size of 64 with learning rate of 0.0005, optimised using the RAdam optimiser and plateau-based learning rate scheduling.

The GAT model maintains the same input and output dimensions but uses a smaller hidden dimension of 128 to compensate for the increased parameter count from the attention mechanisms. We employ 8 attention heads in each GAT layer, with 3 total layers and the same dropout rate of 0.5. Training proceeds with a smaller batch size of 32 and learning rate of 0.0003, reflecting the more complex optimisation landscape of the attention-based model.

Both models employ early stopping with a patience of 10 epochs, balancing the need for convergence against the risk of overfitting. These carefully tuned hyperparameters ensure optimal performance whilst maintaining computational efficiency.


\section{Experiments and Results}
\label{sec:experiments}

\subsection{Experimental Setup}
\label{sec:setup}

\textbf{Data Splitting:} We split each dataset into training (70\%), validation (15\%), and test (15\%) sets, ensuring that the class balance is maintained in each split.

\textbf{Implementation Details:} The models are implemented using PyTorch and PyTorch Geometric. We use the following hyperparameters:
\begin{itemize}
    \item GNN layers: 2-3 layers with 128-256 hidden dimensions
    \item MLP: 2 layers with dropout (0.5)
    \item Batch size: 32-64
    \item Learning rate: 0.0001-0.001 with decay
    \item Training epochs: Up to 100 with early stopping
\end{itemize}

\textbf{Evaluation Metrics:} We evaluate the models using several metrics:
\begin{itemize}
    \item Accuracy: Overall prediction accuracy
    \item Precision: Ratio of true positives to predicted positives
    \item Recall: Ratio of true positives to actual positives
    \item F1 Score: Harmonic mean of precision and recall
    \item AUROC: Area under the Receiver Operating Characteristic curve
    \item AUPRC: Area under the Precision-Recall curve
\end{itemize}

\subsection{Performance Comparison}
\label{sec:performance}

[Table with performance results for GCN and GAT models on different datasets]

\subsection{Analysis of Model Components}
\label{sec:analysis}

\textbf{Impact of Embedding Methods:} We compare different protein embedding methods, including:
\begin{itemize}
    \item ProtBERT (our primary approach)
    \item One-hot encoding of amino acids
    \item Pre-computed ESM embeddings
\end{itemize}

\textbf{Graph Construction Strategies:} We analyze different edge construction methods:
\begin{itemize}
    \item Sequential proximity (used in our approach)
    \item k-nearest neighbors in embedding space
    \item Contact maps from protein structures (where available)
\end{itemize}

\textbf{Attention Analysis:} For the GAT model, we visualize the attention weights to identify which residues are most important for predicting interactions.

\section{Discussion and Biological Insights}
\label{sec:discussion}

\subsection{Model Interpretability}
\label{sec:interpretability}

One advantage of our GAT-based approach is the ability to interpret the attention weights to identify key residues involved in protein interactions. This provides valuable biological insights beyond simple binary classification.

\subsection{COVID-19 Specific Insights}
\label{sec:covid_insights}

For the COVID-19 dataset, we analyze the patterns learned by our model for SARS-CoV-2 protein interactions with human proteins. These insights could contribute to understanding viral mechanisms and identifying potential therapeutic targets.

\subsection{Limitations and Future Directions}
\label{sec:limitations}

We discuss the current limitations of our approach:
\begin{itemize}
    \item Reliance on sequence information without full 3D structural data
    \item Computational challenges with very large proteins
    \item Limited ability to model the dynamics of protein interactions
\end{itemize}

Future directions include:
\begin{itemize}
    \item Incorporating explicit 3D structural information
    \item Developing hierarchical graph representations for computational efficiency
    \item Extending the framework to predict binding affinity and interaction sites
    \item Integrating additional biological data sources, such as gene expression and phylogenetic information
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

In this research, we presented a graph-based deep learning approach for predicting protein-protein interactions. By representing proteins as graphs and leveraging the power of graph neural networks, our method effectively captures both sequence and structural information. The experiments on multiple datasets demonstrate the effectiveness of our approach, particularly the GAT model which provides both strong predictive performance and interpretability.

Our method offers a promising computational tool for large-scale prediction of PPIs, which can help prioritize candidates for experimental validation and contribute to understanding complex biological systems. The application to the COVID-19 dataset demonstrates the potential of our approach in studying host-pathogen interactions, which is crucial for developing therapeutic strategies against infectious diseases.

\bibliographystyle{unsrt}
\begin{thebibliography}{00}

\bibitem{gonzalez2012protein} Gonzalez, M.W. and Kann, M.G., 2012. Protein interactions and disease. PLoS computational biology, 8(12), p.e1002819.

\bibitem{berggard2007methods} Berggård, T., Linse, S. and James, P., 2007. Methods for the detection and analysis of protein–protein interactions. Proteomics, 7(16), pp.2833-2842.

\bibitem{skrabanek2008computational} Skrabanek, L., Saini, H.K., Bader, G.D. and Enright, A.J., 2008. Computational prediction of protein–protein interactions. Molecular biotechnology, 38(1), pp.1-17.

\bibitem{valencia2002computational} Valencia, A. and Pazos, F., 2002. Computational methods for the prediction of protein interactions. Current opinion in structural biology, 12(3), pp.368-373.

\bibitem{chen2019review} Chen, K.H., Wang, T.F., Hu, Y.J., 2019. Protein-protein interaction prediction using deep learning. International Journal of Molecular Sciences, 20(17), p.4146.

\bibitem{shen2007predicting} Shen, J., Zhang, J., Luo, X., Zhu, W., Yu, K., Chen, K., Li, Y. and Jiang, H., 2007. Predicting protein–protein interactions based only on sequences information. Proceedings of the National Academy of Sciences, 104(11), pp.4337-4341.

\bibitem{huang2015scoring} Huang, S.Y., 2015. Exploring the potential of global protein–protein docking: an overview and critical assessment of current programs for automatic ab initio docking. Drug discovery today, 20(8), pp.969-977.

\bibitem{you2013prediction} You, Z.H., Lei, Y.K., Zhu, L., Xia, J. and Wang, B., 2013. Prediction of protein-protein interactions from amino acid sequences with ensemble extreme learning machines and principal component analysis. BMC bioinformatics, 14(8), pp.1-11.

\bibitem{guo2008using} Guo, Y., Yu, L., Wen, Z. and Li, M., 2008. Using support vector machine combined with auto covariance to predict protein–protein interactions from protein sequences. Nucleic acids research, 36(9), pp.3025-3030.

\bibitem{sun2017sequence} Sun, T., Zhou, B., Lai, L. and Pei, J., 2017. Sequence-based prediction of protein protein interaction using a deep-learning algorithm. BMC bioinformatics, 18(1), pp.1-8.

\bibitem{chen2019multifaceted} Chen, M., Ju, C.J.T., Zhou, G., Chen, X., Zhang, T., Chang, K.W., Zaniolo, C. and Wang, W., 2019. Multifaceted protein-protein interaction prediction based on Siamese residual RCNN. Bioinformatics, 35(14), pp.i305-i314.

\bibitem{du2017deepppi} Du, X., Sun, S., Hu, C., Yao, Y., Yan, Y. and Zhang, Y., 2017. DeepPPI: boosting prediction of protein-protein interactions with deep neural networks. Journal of chemical information and modeling, 57(6), pp.1499-1510.

\bibitem{ingraham2019generative} Ingraham, J., Garg, V., Barzilay, R. and Jaakkola, T., 2019. Generative models for graph-based protein design. Advances in Neural Information Processing Systems, 32.

\bibitem{gligorijevic2021structure} Gligorijević, V., Renfrew, P.D., Kosciolek, T., Leman, J.K., Berenberg, D., Vatanen, T., Chandler, C., Taylor, B.C., Fisk, I.M., Vlamakis, H. and Xavier, R.J., 2021. Structure-based protein function prediction using graph convolutional networks. Nature Communications, 12(1), pp.1-14.

\bibitem{fout2017protein} Fout, A., Byrd, J., Shariat, B. and Ben-Hur, A., 2017. Protein interface prediction using graph convolutional networks. Advances in neural information processing systems, 30.

\bibitem{keshava2009human} Keshava Prasad, T.S., Goel, R., Kandasamy, K., Keerthikumar, S., Kumar, S., Mathivanan, S., Telikicherla, D., Raju, R., Shafreen, B., Venugopal, A. and Balakrishnan, L., 2009. Human protein reference database—2009 update. Nucleic acids research, 37(suppl_1), pp.D767-D772.

\bibitem{stark2006biogrid} Stark, C., Breitkreutz, B.J., Reguly, T., Boucher, L., Breitkreutz, A. and Tyers, M., 2006. BioGRID: a general repository for interaction datasets. Nucleic acids research, 34(suppl_1), pp.D535-D539.

\bibitem{orchard2014mintact} Orchard, S., Ammari, M., Aranda, B., Breuza, L., Briganti, L., Broackes-Carter, F., Campbell, N.H., Chavali, G., Chen, C., Del-Toro, N. and Duesbury, M., 2014. The MIntAct project—IntAct as a common curation platform for 11 molecular interaction databases. Nucleic acids research, 42(D1), pp.D358-D363.

\bibitem{salwinski2004database} Salwinski, L., Miller, C.S., Smith, A.J., Pettit, F.K., Bowie, J.U. and Eisenberg, D., 2004. The database of interacting proteins: 2004 update. Nucleic acids research, 32(suppl_1), pp.D449-D451.

\bibitem{cherry2012saccharomyces} Cherry, J.M., Hong, E.L., Amundsen, C., Balakrishnan, R., Binkley, G., Chan, E.T., Christie, K.R., Costanzo, M.C., Dwight, S.S., Engel, S.R. and Fisk, D.G., 2012. Saccharomyces Genome Database: the genomics resource of budding yeast. Nucleic acids research, 40(D1), pp.D700-D705.

\bibitem{oughtred2021biogrid} Oughtred, R., Rust, J., Chang, C., Breitkreutz, B.J., Stark, C., Willems, A., Boucher, L., Leung, G., Kolas, N., Zhang, F. and Dolma, S., 2021. The BioGRID database: a comprehensive biomedical resource of curated protein, genetic, and chemical interactions. Protein Science, 30(1), pp.187-200.

\bibitem{elnaggar2021prottrans} Elnaggar, A., Heinzinger, M., Dallago, C., Rehawi, G., Wang, Y., Jones, L., Gibbs, T., Feher, T., Angerer, C., Steinegger, M. and Bhowmik, D., 2021. ProtTrans: toward understanding the language of life through self-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 44(10), pp.7112-7127.

\end{thebibliography}

\end{document}